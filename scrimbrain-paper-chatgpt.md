# PROMPT

Write a scientific paper using the open-source repo at https://github.com/wkwan/scrimbrain and YouTube video (https://youtu.be/iFAe7x2Nos8?si=tt6N-BSeDrii5TS7) which explains several experiments run with the codebase. The structure of the paper should roughly be:

1. Intro (include a brief literature review)
2. Methodology (describe approach and hypothesis)
3. ⁠Experiment (formalize the explanation in the YouTube video transcript)
4. ⁠Result (leave room for figures to be added later. these could include the Tensorboard graphs and the visualizations of the convolutional filters generated by visualize_model.py)
⁠5. Discussion
⁠6. Conclusion

Here's the transcript of the YouTube video:
in this video I'm going to go over a few
AIS that I developed to play three
simple fortnite mini gamess I've thought
for years that training neural Nets to
play games would be very useful to
automate some play testing for game
developers and help competitive Gamers
discover new strategies especially when
a big patch rolls out and everyone is
racing to find the new meta that's why 4
years ago I made the AI learns to play
Mortal Kombat video as a proof of
concept and a few months after that I
tried to do the same thing for League of
Legends but it was really hard so I
thought okay I can wait a few years for
the research to improve and companies to
build useful tools that I can use myself
as an indie game developer in Esports
nerd well fast forward four years and
the branch of AI used to train game
agents actually hasn't improved very
much this branch of AI is called Deep
reinforcement learning and it works by
feeding the current state of the game to
the model often in the form of a stack
of recent frames the model predicts what
act the AI should take and then the
programmer writes something called a
reward function that evaluates how good
or bad the actions were in a game like
Mario the reward function could be as
simple as a small positive number when
Mario moves to the right and a big
negative number when Mario dies after
spending enough time training and
tweaking the setup AI will hopefully
learn to choose actions to maximize
reward even though open Ai and Deep Mind
have beaten top Pros in Dota and
Starcraft years ago nobody took this
research and made useful tools for
competitive Gamers and game developers
nobody even made it easier for the
average developer to build stuff with
this AI so I decide to put together a
simple and well-documented python
project teaching AI to play fortnite
through real-time screen capture let's
go over a few models I trained and talk
about some useful things that you can do
with this code but first in case you've
been MIA for the last 7 years fortnite
is a battle royale where 100 players
fight on an island to be the last player
or team standing while looting for
weapons items and building materials in
the last year fortnite made their
modding tools way better so I've been
making these fortnite custom games
called Maps the top maps have as many
concurrent players as a top 50 game on
Steam but don't worry I'm not in this
for the attention I just love beating up
teenage boys he snuck up on me currently
there's no way to train AI in fortnite
and even though it'd be very hard to get
AI to learn the main battle royale mode
I think I can train something simple
that can be useful for play testing my
maps and maybe it'll give me some ideas
for how to trade more complex AIS that
could be used as Esports practice so I
set out to be the first person to get
reinforcement learning working in
fortnite I started this project in
February and put together what I thought
was an easy game mode for AI to learn
these are two separate neuron Nets at
the beginning of training fighting each
other with an assault rifle and infinite
ammo the reason it's two neuron Nets is
because a common technique when Training
Game AI is self-play where AI learns by
playing against itself self or previous
versions of itself this works because as
the AI improves the opponent also
improves so the game difficulty
increases during training I didn't do
self-play because fortnite doesn't let
you run two instances of fortnite on the
same PC and I wanted to avoid sinking
models across multiple PCS in my
prototype but I thought I could achieve
a similar result by running my training
script separately on two PCS having them
fight each other while both AIS improve
independently if you've played fortnite
before you probably noticed that I
disabled the main mechanic fortnite is
known for building and editing if you
don't know what that is it looks like
this oh my God at the time I had just
launched my first fortnite map guard
Commander my background is in unity but
many of the top orts started off as mods
of other games which allowed the
developers to launch faster and leverage
an existing Community my goal is to
build a top Esports so I got into
fortnite modding because it seems like
it is the best developer ecosystem the
unreal editor for fortnite was easy for
me to pick up actually the hardest part
about making guard Commander was
learning how to play fortnite Rec
clicking the left button that's what's
killing me yes it's it's an odd mechanic
to start there's not a lot of games
where you do that specifically the
building and editing mechanic I was
grinding ranked in Battle Royale for a
few days and even though I had smooth
sailing from bronze to Plat I was
heavily relying on my strategy of
running away from everybody with an
occasional lucky kill and and then I
would make into end game and die because
in end game you actually have to be
decent at building and I sucked at
it I figured if I was having a hard time
learning this mechanic then it would
probably also be hard for AI so I
disabled it during training if I wanted
AI to learn interesting fortnite
strategies then I would need to enable
building but I wasn't expecting the
first version to be good enough for
players to practice against this was
supposed to be a proof of concept to see
if the competitive Community would be
interested and also to see if I could
use the AI myself to automate some play
testing for my maps to find bugs and
come up with ideas besides this AI I've
been working with my friend coach Dodie
on a 1v1 offense versus defense map
called steal my
wall where the attacker must kill the
defender and the defender must survive
the time limit while staying inside
their box I'm the weakest play tester we
have and I still find bugs or things to
change every time I play so the bar is
very low for AI to be useful when I
first thought about AI play testing 4
years ago I was living in Toronto
now I'm in San Francisco and about to
move out living in the generative AI
Echo chamber here makes me wonder if all
the hype around other branches of AI
have actually slowed down reinforcement
learning
progress I made the first training map a
small box and gave both AI one HP so
there would be lots of killing and then
I use text recognition to detect when a
player is eliminated this gives a reward
or a punishment to the AI so it learns
to maximize killing and minimize dying I
ran multiple experiments with this first
setup but all of them had the same
problem after a few hours it would learn
to hug the walls in the corners which is
good because it's more protected and
gives a better view of the rest of the
map but aiming was terrible and after a
few more hours it would get more
terrible one of my experiments was to
give AI a small reward when it's facing
the other player I did this by using the
YOLO object detection model to detect
the other player model I had to make
sure that the YOLO model would not
activate when it only detects the
current player so that's why one of the
players is not a human but a chicken I
was very proud of this scientific
breakthrough but ultimately it didn't
make the AI any smarter so I needed to
kill the
chicken I knew I had to make big changes
but it took a month and a half to get to
this point and I was determined to get
my chicken dinner so I took what I had
and did some user testing so the game is
you have one in it to kill the chicken
as many times as you can your chicken is
the bot the chicken is me once we start
the game what I'm going to do is I'm
going to load up an external Python
program and then it's going to start
simulating keyboard and mouse inputs let
me just figure out how to shoot again
one yeah there you go all right let's go
let's running bro I'm ready a Crosshair
is on this chicken's head oh oh shoot oh
chicken what the oh dude this guy's
shooting the roof
what okay that that was pretty much
it it didn't find
me it wasn't it felt like it wasn't
trying to look at me two obstacles
differentiated my project from most game
AI research the first is that I'm
training on a single real-time instance
of fortnite typically complex game AI
either train on multiple instances of a
game in parallel or in a sped up
instance of game the second obstacle is
that there's no way to explicitly pass
data from fortnite to my P torch model
which is why I'm using screen capture I
knew that if I built a game in unity or
Unreal Engine it would be way easier to
train AI to play it but if I wanted to
use AI to play test my fortnite maps and
if I wanted to eventually train AI on
popular closed Source Esports I don't
see a better solution that doesn't have
these obstacles so in the next set of
experiments I removed unnecessary
Parts while still training on a single
real-time instance of fortnite and still
using screen capture instead of two
neuron Nets we now have one neuronet
fighting against the default guard I
disabled moving the Crosshair up and
down because the guard and the player
are always at ground level and I also
disabled movement so all the AI can do
is Pivot horizontally jump Crouch and
shoot intuitively reducing the possible
actions should make it easier and faster
for AI to learn something reasonable I
still thought that having a small reward
for facing the enemy could help training
even though I previously saw no benefit
with the YOLO object detection so I
implemented a simpler version of this
which uses image detection for the
visualized audio icon to give AI a small
reward when there are Footsteps in front
of the AI that a human would be able to
hear I streamed multiple training
experiments on Twitch including this one
which I ran for 45 hours but it still
suck so I made another even simpler
map SHO shooting requires Precision so I
got rid of it the goal of the new map is
to run to the metal Square as fast as
you can once you reach it it teleports
to another random location I permanently
pressed down the W key to move the AI
forward and disabled other movement Keys
AI can only choose between one of three
actions look left look straight or look
right I also switched to using the dqn
algorithm because it's supposed to be
better at learning with low amounts of
data finally after 2 months of
experimentation it worked
the easiest way to tell is that the
average reward kept increasing for the
first two days of training which means
AI kept reaching metal squares faster
and faster it's not as good as me but
it's clearly turning to face the metal
squares and it has no hard-coded
knowledge about what these metal squares
are AI was trained by getting a reward
when this blue text appears yes I could
have hardcoded it to move towards the
middle trigger but the fact that it
figured this out on its own through
computer vision of reinforcement
learning means that it'll be able to
figure out more more complex tasks that
I can't hardcode like building and
editing on GitHub I've documented how to
run the model how to train your own
model and how to visualize the
processing that the model is doing on
the screenshots you might be wondering
from the first experiment to this one
which one of the changes was most
important for getting AI to actually
learn something and I don't know but
here's everything I tried the next step
is to train it on a real fortnite map
and the best map I've worked on is steal
my
wall I like competitive multiplayer
games that are easy to pick up but hard
to masteress up steel my wall is a
competitive map that helps you practice
an essential skill that's hard to learn
in other Maps we first designed it for
top fortnite players got great feedback
and now we're adding tutorials for the
average player like me this has been a
2-month project and between this and the
AI I haven't had much time to play
fortnite so it's time to start
practicing build a wall bottom left
corner edit let's start with some bu
here you can practice taking good Peaks
from different positions most rely on
the strategy of right hand peeking which
means you usually want to build cover on
your left so you can Peak over on the
right and shoot the enemy while they
can't see you build wall right window
edit wall you can only edit a piece that
you place yourself so a common strategy
is to steal your opponent's build by
destroying it and immediately placing
your own stealing is the main mechanic
in our game so we set up this tutorial
section with a bunch of different
scenarios where you might want to steal
a piece and we've got audio guides and
screenshots to tell you how to do it
this diagonal setup from above is one of
the most common ways to steal a wall in
Pro gameplay this is a very difficult
angle for your opponent to hit a shot on
you from another strength to this setup
is you have multiple options if they do
happen to hit a shot on you you can
reset the wall the flat the roof you can
also create even more distance between
you and your opponent by editing the
roof and going up to take height once
you've done the solo practice areas
you're ready for the main game 1 V one
attacker versus defender in seven
scenarios it'll probably still be hard
and that's intentional I did all the
programming for the map and I'm still
bad at stealing pieces oh we got a
chance here that's why I teamed up with
Coach Dodie who did most of the game
design and while we got insanely skilled
play testers AI is never going to
replace human play testing but it can
automate the boring parts and
reinforcement learning is the easiest
way to do it

# RESPONSE

## Title: Reinforcement Learning for Fortnite: Experiments with ScrimBrain

### Authors:

### Abstract
This paper explores the application of reinforcement learning to train AI agents in Fortnite using the ScrimBrain codebase. We conduct several experiments as a first step towards using reinforcement learning for automated playtesting and esports practice tools. The results demonstrate the challenges and potential of reinforcement learning in dynamic game environments.

### 1. Introduction
#### Literature Review
Reinforcement learning (RL) has been a prominent method in training AI agents for games. Notable successes include DeepMind's AlphaGo and OpenAI's Dota 2 bots, which have defeated top pros. However, practical tools for game developers and competitive gamers remain limited. This paper aims to bridge this gap by utilizing the ScrimBrain codebase to train AI agents for Fortnite, a complex and popular battle royale game.

### 2. Methodology
#### Approach
Our approach involves training neural networks to play Fortnite by using real-time screen capture, Windows input simulation, and reinforcement learning techniques. We hypothesize that AI can be effectively trained to perform specific in-game tasks, despite the complexities of the game environment.

#### Hypothesis
We hypothesize that:
1. AI can learn to perform basic actions in Fortnite through reinforcement learning.
2. Simplifying the action space will improve learning efficiency.
3. A carefully written reward function is criticial for learning.

### 3. Experiments
#### Experimental Setup
The experiments were conducted using the ScrimBrain codebase. The experiments aimed to:
1. Train AI agents to maximize their survival and combat effectiveness.
2. Evaluate the impact of various reward functions and training configurations.

#### Experiment 1: Initial Training
- **Objective**: Train AI agents in a simplified environment.
- **Setup**: The initial setup involved 2 neural networks fighting each other in a simplified Fortnite environment with disabled building mechanics. The AI agents had infinite ammo and fought in a small box with 1 HP each.
- **Outcome**: The AI agents learned to hug walls and corners but showed poor aiming capabilities.

#### Experiment 2: Facing the Enemy
- **Objective**: Improve AI aiming by rewarding facing the enemy.
- **Setup**: Same as Experiment 1, but also used YOLO object detection to reward facing the opponent.
- **Outcome**: No significant improvement in AI performance.

#### Experiment 3: Simplify Action Space and Only Use 1 Neural Network
- **Objective**: Simplify the action space to make learning easier. Instead of using 2 neural networks (which requires 2 PC's because Fortnite anti-cheat prevents a single PC from running 2 instances of Fortnite), use a signle neural network and have them fight against the built-in Fortnite bot.
- **Setup**: AI can only pivot horizontally, jump, crouch, and shoot. Used image detection for audio cues.
- **Outcome**: Slight improvement, but AI still didn't show clear clear signs of learning.

#### Experiment 4: Running to a Target
- **Objective**: Train AI to run towards a target.
- **Setup**: AI moves forward continuously and can only look left, right, or straight. Used DQN algorithm which has been shown to be more sample efficient.
- **Outcome**: Successful training, with AI learning to reach targets efficiently.

### 4. Results
#### Tensorboard Graphs
- Placeholder for Tensorboard graphs showcasing the training progress and reward trends.

#### Visualizations
- Placeholder for visualizations generated by `visualize_model.py`, illustrating the learned behavior of AI agents.

### 5. Discussion
The experiments highlight the challenges in training AI for complex games like Fortnite. Simplifying the action space and using targeted rewards were crucial for achieving reasonable performance. The use of real-time screen capture and reinforcement learning proved feasible but required significant computational resources.

### 6. Conclusion
This study demonstrates the potential of reinforcement learning in training AI agents for Fortnite. While the initial results are promising, further research is needed to handle more complex tasks, such as building and editing. The ScrimBrain codebase provides a foundation for future work in this area, with potential applications in automated playtesting and esports practice tools.